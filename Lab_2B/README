#NAME:Liuyi Shi
#EMAIL:liuyi.shi@outlook.com
#ID:904801945

Files Descriptions: 
SortedList.h - header file of sorted list 
SortedList.c - implementation of sorted list 
lab2_list.c - main functions 
Makefile -makefile and many other functions 
lab2b_list.csv - output result 
profile.out - profile of where time are spent 
lab2b_1.png - throughput vs. number of threads for mutex and spin-lock synchronized list operations.
lab2b_2.png - mean time per mutex wait and mean time per operation for mutex-synchronized list operations.
lab2b_3.png - successful iterations vs. threads for each synchronization method.
lab2b_4.png - throughput vs. number of threads for mutex synchronized partitioned lists.
lab2b_5.png - throughput vs. number of threads for spin-lock-synchronized partitioned lists.
README  - this file 


QUESTION 2.3.1 - Cycles in the basic list implementation:
Where do you believe most of the cycles are spent in the 1 and 2-thread list tests ?
    They go to list insertion, lookup and delets.
Why do you believe these to be the most expensive parts of the code?
    Becasue there aren't many threads for it to wait for. 
Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?
    I think most of the time are being spent on spinning of each thread. 
Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?
    I think for mutex, most of its time are being spent on doing system call including blocking and context swith. 

QUESTION 2.3.2 - Execution Profiling:
Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?
    At spinlock routine. Functions, __sync_lock_test_and_set consumes almost all the cycles. 
Why does this operation become so expensive with large numbers of threads?
    Because while one thread is executing, other threads must spin and wait for it.

QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?
    Beacause the increasing in threads will cause more contention. Every threads need to wait longer time in order to get the resources they need. 
Why does the completion time per operation rise (less dramatically) with the number of contending threads?
    Because the thread number increases as well. Thus the inrease in waiting time is amortized by the increasing number in threads. 
How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?
    Because you add up the time of all threads. When multiple threads are paralliing, completion time will only count it once, but the wait time per operation will count it multiple times.

QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.
    With more sub lists, we are able to gain an increase in throughput. Becasue each list's size is decreased, the workload for each thread is decreased. 
Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
    No. Becasue the smallest size of each list is limited. It cannot go infiinitely small. However, with a list too small, the number of lists will become very large. The space complexity will become our main concern. 
It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.
    Yes. It does seem true to be in my curves. 

